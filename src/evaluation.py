"""RAGAS evaluation module for agent testing."""

import json
import time
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    answer_correctness,
    faithfulness,
    context_precision,
    answer_relevancy
)


# Test queries and reference answers
TEST_CASES = [
    {
        "question": "Compare revenue growth Q3 FY2025: Accenture.pdf vs Siemens.pdf. Which higher?",
        "ground_truth": "Accenture: +8% USD [Accenture.pdf, p.1], Siemens: +3% actual [Siemens.pdf, p.7]. Accenture higher."
    },
    {
        "question": "Operating margins Q3 FY2025 for Accenture.pdf, Siemens.pdf, Infineon.pdf?",
        "ground_truth": "Infineon: 18.0% [Infineon.pdf, p.1], Accenture: 16.8% [Accenture.pdf, p.2], Siemens: 14.9% [Siemens.pdf, p.7]"
    },
    {
        "question": "Timeline of Q3 FY2025 earnings releases?",
        "ground_truth": "Accenture: June 20, 2025 [Accenture.pdf, p.1], Infineon: August 5, 2025 [Infineon.pdf, p.1], Siemens: August 7, 2025 [Siemens.pdf, p.1]"
    },
    {
        "question": "What was Siemens.pdf Industrial Business revenue in Q3 FY2025?",
        "ground_truth": "€18.5B, +3% actual growth [Siemens.pdf, p.7]"
    },
    {
        "question": "Compare Q3 FY2025 free cash flow across all docs.",
        "ground_truth": "Accenture: $3.52B [Accenture.pdf, p.3], Siemens: €2.9B [Siemens.pdf, p.7], Infineon: €288M [Infineon.pdf, p.12]. Accenture highest, Siemens second, Infineon lowest."
    },
    {
        "question": "Segment Result Margin for Infineon.pdf Q3 FY2025?",
        "ground_truth": "18.0% [Infineon.pdf, p.1]"
    },
    {
        "question": "Employees end of Q3 FY2025: Siemens.pdf vs Infineon.pdf?",
        "ground_truth": "Siemens: 315,000 [Siemens.pdf, p.6], Infineon: 56,371 [Infineon.pdf, p.9]"
    },
    {
        "question": "Accenture.pdf revenue change Q3 FY2025 vs Q3 FY2024?",
        "ground_truth": "+8% USD, +7% local currency [Accenture.pdf, p.1]"
    },
    {
        "question": "Siemens.pdf net income Q3 FY2025?",
        "ground_truth": "€2.243B [Siemens.pdf, p.8]"
    },
    {
        "question": "Infineon.pdf adjusted gross margin Q3 FY2025?",
        "ground_truth": "43.0% [Infineon.pdf, p.1]"
    }
]


def collect_agent_responses(agent, test_cases: List[Dict[str, str]]) -> List[Dict[str, Any]]:
    """
    Collect responses from agent for test cases.

    Args:
        agent: MultiDocumentAgent instance
        test_cases: List of test case dictionaries with 'question' and 'ground_truth'

    Returns:
        List of dictionaries with question, answer, contexts, ground_truth, and timing info
    """
    agent_data = []

    for idx, test_case in enumerate(test_cases, 1):
        question = test_case["question"]
        ground_truth = test_case["ground_truth"]

        print(f"Processing test case {idx}/{len(test_cases)}...")

        # Measure response time
        start_time = time.time()
        response = agent.query(question, stream=False)
        latency = time.time() - start_time

        messages = response["messages"]

        # Extract final answer, contexts, and sources
        answer = ""
        contexts = []
        sources_cited = []
        tool_calls = []

        for msg in messages:
            # Get AI response (only the final non-tool-call message)
            if msg.type == "ai" and msg.content:
                if not hasattr(msg, 'tool_calls') or not msg.tool_calls:
                    answer = msg.content
                elif hasattr(msg, 'tool_calls') and msg.tool_calls:
                    # Track tool calls
                    for tc in msg.tool_calls:
                        tool_calls.append({
                            "name": tc.get("name", "unknown"),
                            "args": tc.get("args", {})
                        })

            # Get tool responses (contexts)
            elif msg.type == "tool":
                if msg.content:
                    if isinstance(msg.content, str):
                        contexts.append(msg.content)
                        # Extract source citations
                        sources_cited.extend(_extract_sources(msg.content))
                    elif isinstance(msg.content, list):
                        contexts.extend([str(c) for c in msg.content if c])
                    else:
                        contexts.append(str(msg.content))

        # Ensure contexts is non-empty
        if not contexts:
            contexts = ["No relevant context was retrieved for this query."]

        # Ensure answer is not empty
        if not answer:
            answer = "No answer was generated by the agent."

        agent_data.append({
            "question": question,
            "answer": answer,
            "contexts": contexts,
            "ground_truth": ground_truth,
            "latency_seconds": round(latency, 2),
            "sources_cited": list(set(sources_cited)),  # Remove duplicates
            "tool_calls": tool_calls
        })

    return agent_data


def _extract_sources(context: str) -> List[str]:
    """Extract source citations from context string."""
    import re
    sources = []
    # Match pattern: [Document: filename.pdf | Page: X]
    matches = re.findall(r'\[Document: ([^\]]+\.pdf) \| Page: (\d+|\?)\]', context)
    for doc, page in matches:
        sources.append(f"{doc}:page{page}")
    return sources


def evaluate_agent(agent, test_cases: List[Dict[str, str]] = None, model_name: str = "gpt-4.1") -> Dict[str, Any]:
    """
    Evaluate agent using RAGAS metrics.

    Args:
        agent: MultiDocumentAgent instance
        test_cases: Optional list of test cases (uses default if not provided)
        model_name: Name of the model being evaluated

    Returns:
        Dictionary with full evaluation results including metadata and test cases
    """
    if test_cases is None:
        test_cases = TEST_CASES

    print(f"Collecting responses for {len(test_cases)} test cases...")
    agent_data = collect_agent_responses(agent, test_cases)

    # Convert to dataset format for RAGAS
    dataset_dict = {
        "question": [item["question"] for item in agent_data],
        "answer": [item["answer"] for item in agent_data],
        "contexts": [item["contexts"] for item in agent_data],
        "ground_truth": [item["ground_truth"] for item in agent_data]
    }

    eval_dataset = Dataset.from_dict(dataset_dict)

    print(f"\nRunning RAGAS evaluation...")
    ragas_results = evaluate(
        dataset=eval_dataset,
        metrics=[
            answer_correctness,  # How correct is the answer vs ground truth
            faithfulness,        # Is answer faithful to retrieved context
            context_precision,   # Are retrieved contexts relevant
            answer_relevancy     # Is answer relevant to question
        ]
    )

    # Convert RAGAS results to pandas DataFrame
    results_df = ragas_results.to_pandas()

    # Build minimal results JSON - include ALL test cases
    test_cases_results = []

    for idx, row in enumerate(agent_data):
        # Get RAGAS scores
        correctness = float(results_df.iloc[idx]['answer_correctness'])
        faithfulness_score = float(results_df.iloc[idx]['faithfulness'])
        precision = float(results_df.iloc[idx]['context_precision'])
        relevancy = float(results_df.iloc[idx]['answer_relevancy'])

        test_case = {
            "query": row["question"],
            "expected": row["ground_truth"],
            "actual": row["answer"],
            "scores": {
                "correctness": round(correctness, 3),
                "faithfulness": round(faithfulness_score, 3),
                "precision": round(precision, 3),
                "relevancy": round(relevancy, 3)
            }
        }

        test_cases_results.append(test_case)

    # Build final results
    results = {
        "model": model_name,
        "date": datetime.utcnow().isoformat() + "Z",

        "metrics": {
            "correctness": round(results_df['answer_correctness'].mean(), 3),
            "faithfulness": round(results_df['faithfulness'].mean(), 3),
            "precision": round(results_df['context_precision'].mean(), 3),
            "relevancy": round(results_df['answer_relevancy'].mean(), 3)
        },

        "test_cases": test_cases_results
    }

    # Print results
    print("\n" + "="*60)
    print("RAGAS EVALUATION")
    print("="*60)
    print(f"Correctness:  {results['metrics']['correctness']:.3f}")
    print(f"Faithfulness: {results['metrics']['faithfulness']:.3f}")
    print(f"Precision:    {results['metrics']['precision']:.3f}")
    print(f"Relevancy:    {results['metrics']['relevancy']:.3f}")
    print("="*60)

    return results


def save_results(results: Dict[str, Any], output_dir: str = "results") -> str:
    """
    Save evaluation results to JSON file.

    Args:
        results: Evaluation results dictionary
        output_dir: Directory to save results

    Returns:
        Path to saved file
    """
    # Create results directory
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    # Generate filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"evaluation_results_{timestamp}.json"
    filepath = output_path / filename

    # Save to JSON
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    return str(filepath)
